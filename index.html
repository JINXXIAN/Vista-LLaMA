<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Vista_LLaMA</title>
  <link rel="icon" type="image/x-icon" href="static/images/Vista-LLaMA-icon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
        .title.is-2.has-text-centered {
            margin-top: 28px; 
        }
  </style>


  <!-- CSS 样式规则，用于居中图片 -->
  <style>
        .centered-image {
            display: block;
            margin: 0 auto; /* 设置左右外边距为自动以水平居中图片 */
            max-width: 70%; /* 确保图片不超出其容器的宽度 */
        }
  </style>

  
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/Vista-LLaMA-icon.png" alt="Image Description" style="width: 75px; height: auto;">
            <h1 class="title is-1 publication-title">Vista-LLaMA:</h1>
            <h1 class="title is-1 publication-title">Reliable Video Teller via Equal Distance to Visual Tokens</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Fan Ma</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Heng Wang</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yuchen Xian</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Jiashi Feng</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="FIFTH AUTHOR PERSONAL LINK" target="_blank">Xiaojie Jin</a><sup>2*</sup>,</span>
                        <span class="author-block">
                          <a href="SIXTH AUTHOR PERSONAL LINK" target="_blank">Yi Yang</a><sup>1*</sup>
                        </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Zhejiang University&nbsp;&nbsp;<sup>2</sup>ByteDance Inc.</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author.</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-three-quarters">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified" style="max-width: 100%;">
          <p>
            Recent advances in large video-language models have displayed promising outcomes in video comprehension. Current approaches straightforwardly convert video into language tokens and employ large language models for multi-modal tasks.
However, this method often leads to the generation of irrelevant content, commonly known as ``hallucination'', as the length of the text increases and the impact of the video diminishes.
To address this problem, we propose Vista-LLaMA, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length.
Vista-LLaMA omits relative position encoding when determining attention weights between visual and text tokens, retaining the position encoding for text and text tokens. This amplifies the effect of visual tokens on text generation, especially when the relative distance is longer between visual and text tokens. The proposed attention mechanism significantly reduces the chance of producing irrelevant text related to the video content.
Furthermore, we present a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame. This approach not only captures the temporal relationship within the video, but also allows less visual tokens to encompass the entire video.
Our approach significantly outperforms various previous methods(e.g., Video-ChatGPT, MovieChat) on four challenging open-ended video question answering benchmarks. 
We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot MSRVTT-QA, setting a new state-of-the-art performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
        <h2 class="title is-2 has-text-centered">Our Framework</h2>
        <!-- Your image here -->
        <img src="static/images/banner.png" alt="MY ALT TEXT" class="centered-image">
        <h2 class="subtitle" style="max-width: 75%; margin: 0 auto; text-align: left;">
          <br>The framework of <strong>Vista-LLaMA</strong>. The visual encoder and large language model are both frozen during training, while the projector is trainable to map video into the language's space. The attention operation in each layer is present on the right part. Only the text tokens are applied with rotary position embedding to include relative distance information. The attention weights between visual and language tokens are calculated without the rotary position embedding. The casual mask is applied to the bottom-right attention weights. 
</h2>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
        <h2 class="title is-2 has-text-centered">Comparison with SOTA</h2>
        <!-- Your image here -->


</div>
</div>
</section>

<table>
  <thead>
    <tr>
      <th rowspan="2">Method</th>
      <th colspan="2">NExT-QA</th>
      <th colspan="2">MSVD-QA</th>
      <th colspan="2">MSRVTT-QA</th>
      <th colspan="2">ActivityNet-QA</th>
    </tr>
    <tr>
      <th>Accuracy</th>
      <th>Score</th>
      <th>Accuracy</th>
      <th>Score</th>
      <th>Accuracy</th>
      <th>Score</th>
      <th>Accuracy</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>FrozenBiLM~(yang2022zero)</td>
      <td>-</td>
      <td>-</td>
      <td>32.2</td>
      <td>-</td>
      <td>16.8</td>
      <td>-</td>
      <td>24.7</td>
      <td>-</td>
    </tr>
    <tr>
      <td>Video Chat~(li2023videochat)</td>
      <td><u>56.2</u></td>
      <td><u>3.2</u></td>
      <td>56.3</td>
      <td>2.8</td>
      <td>45.0</td>
      <td>2.5</td>
      <td>26.5</td>
      <td>2.2</td>
    </tr>
    <tr>
      <td>LLaMA Adapter~(zhang2023llama)</td>
      <td>-</td>
      <td>-</td>
      <td>54.9</td>
      <td>3.1</td>
      <td>43.8</td>
      <td>2.7</td>
      <td>34.2</td>
      <td><u>2.7</u></td>
    </tr>
    <tr>
      <td>Video LLaMA~(zhang2023video)</td>
      <td>-</td>
      <td>-</td>
      <td>51.6</td>
      <td>2.5</td>
      <td>29.6</td>
      <td>1.8</td>
      <td>12.4</td>
      <td>1.1</td>
    </tr>
    <tr>
      <td>MovieChat~(song2023moviechat)</td>
      <td>49.9</td>
      <td>2.7</td>
      <td>61.0</td>
      <td>2.9</td>
      <td><u>49.7</u></td>
      <td><u>2.8</u></td>
      <td><b>51.5</b></td>
      <td>3.1</td>
    </tr>
    <tr>
      <td>Video-ChatGPT~(maaz2023video)</td>
      <td>54.6</td>
      <td><u>3.2</u></td>
      <td><u>64.9</u></td>
      <td><u>3.3</u></td>
      <td>49.3</td>
      <td><u>2.8</u></td>
      <td>35.2</td>
      <td><u>2.7</u></td>
    </tr>
    <tr>
      <td><b>Model (Ours)</b></td>
      <td><b>60.7</b></td>
      <td><b>3.4</b></td>
      <td><b>65.3</b></td>
      <td><b>3.6</b></td>
      <td><b>60.5</b></td>
      <td><b>3.3</b></td>
      <td><u>48.3</u></td>
      <td><b>3.3</b></td>
    </tr>
  </tbody>
</table>


<!-- 第一组 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-2 has-text-centered">Our Results</h2>
      <!-- Your image here -->
      <img src="static/images/result.png" alt="MY ALT TEXT" class="centered-image">
      <h2 class="subtitle" style="max-width: 75%; margin: 0 auto; text-align: left;">
        <br><strong>Visualization results</strong> on different video questions. The questions and annotated answers are located on the left side. The generated text from Video-ChatGPT and our model is presented in the green and orange boxes, respectively. Read our paper for more details.
      </h2>
    </div>
  </div>
</section>

<!-- 添加间距 -->
<div style="margin-bottom: 20px;"></div>

<!-- 第二组 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Your image here -->
      <img src="static/images/result1.png" alt="MY ALT TEXT" class="centered-image">
      <h2 class="subtitle" style="max-width: 75%; margin: 0 auto; text-align: left;">
        <br><strong>Visualization results</strong> on different video questions.
      </h2>
    </div>
  </div>
</section>

<!-- 添加间距 -->
<div style="margin-bottom: 20px;"></div>

<!-- 第三组 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Your image here -->
      <img src="static/images/result2.png" alt="MY ALT TEXT" class="centered-image">
      <h2 class="subtitle" style="max-width: 75%; margin: 0 auto; text-align: left;">
        <br><strong>Visualization results</strong> on different video questions.
      </h2>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-2 has-text-centered">CineClipQA Dataset</h2>
      <!-- Your image here -->
      <img src="static/images/dataset.png" alt="MY ALT TEXT" class="centered-image">
      <h2 class="subtitle" style="max-width: 75%; margin: 0 auto; text-align: left;">
        <br><strong>CineClipQA Dataset</strong> to be proposed for movie QA. It is a novel dataset meticulously crafted to probe the capabilities of visual language models in comprehending and interpreting plot-driven video content.
      </h2>
    </div>
  </div>
</section>  

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
           
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
